---
---
@inproceedings{cheema2025describepro,
  title={DescribePro: Collaborative Audio Description with Human-AI Interaction},
  author={Cheema, Maryam S and Elahimanesh, Sina and Martin, Samuel and Fazli, Pooyan and Seifi, Hasti},
  booktitle={Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--19},
  year={2025},
  abbr={ASSETS},
  preview={describePro.gif},
  selected=true,
  url={https://dl.acm.org/doi/10.1145/3663547.3746320},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3663547.3746320},
  abstract = {Audio description (AD) makes video content accessible to millions of blind and low vision (BLV) users. However, creating high-quality AD involves a trade-off between the precision of human-crafted descriptions and the efficiency of AI-generated ones. To address this, we present DescribePro  a collaborative AD authoring system that enables describers to iteratively refine AI-generated descriptions through multimodal large language model prompting and manual editing. DescribePro also supports community collaboration by allowing users to fork and edit existing ADs, enabling the exploration of different narrative styles. We evaluate DescribePro with 18 describers (9 professionals and 9 novices) using quantitative and qualitative methods. Results show that AI support reduces repetitive work while helping professionals preserve their stylistic choices and easing the cognitive load for novices. Collaborative tags and variations show potential for providing customizations, version control, and training new describers. These findings highlight the potential of collaborative, AI-assisted tools to enhance and scale AD authorship.},
}

@inproceedings{cheema2025describe,
  title={Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals},
  author={Cheema, Maryam and Seifi, Hasti and Fazli, Pooyan},
  booktitle={Proceedings of the 2025 ACM Designing Interactive Systems Conference},
  pages={458--474},
  year={2025},
  abbr={DIS},
  selected=true,
  preview={describeNow.gif},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3715336.3735685?casa_token=jdDpA0OutWkAAAAA:9nay7jz0rYRzJN1SJM5VD_0k36yRq6sVjappXEw6jAtdzYnUPvbKRm2c2fHE9KuZsxC1qxodvKvj},
  abstract = {Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants’ sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.},
 }

@inproceedings{li2025videoa11y,
  title={Videoa11y: Method and dataset for accessible video description},
  author={Li, Chaoyu and Padmanabhuni, Sid and Cheema, Maryam S and Seifi, Hasti and Fazli, Pooyan},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--29},
  year={2025},
  abbr={CHI},
  url={https://dl.acm.org/doi/10.1145/3706598.3714096},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3706598.3714096?casa_token=31r7E-rrMHgAAAAA:VcK8TsSeasMXnxiAepnR5dLpTmB4G4zkDag5_O-k5pExYl3XgEqCMWMCdxRYBRZTf4iD4MiqpUIG},
  abstract = {Video descriptions are crucial for blind and low vision (BLV) users to access visual content. However, current artificial intelligence models for generating descriptions often fall short due to limitations in the quality of human annotations within training datasets, resulting in descriptions that do not fully meet BLV users’ needs. To address this gap, we introduce VideoA11y, an approach that leverages multimodal large language models (MLLMs) and video accessibility guidelines to generate descriptions tailored for BLV individuals. Using this method, we have curated VideoA11y-40K, the largest and most comprehensive dataset of 40,000 videos described for BLV users. Rigorous experiments across 15 video categories, involving 347 sighted participants, 40 BLV participants, and seven professional describers, showed that VideoA11y descriptions outperform novice human annotations and are comparable to trained human annotations in clarity, accuracy, objectivity, descriptiveness, and user satisfaction. We evaluated models on VideoA11y-40K using both standard and custom metrics, demonstrating that MLLMs fine-tuned on this dataset produce high-quality accessible descriptions. Code and dataset are available at https://people-robots.github.io/VideoA11y/.},
  booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  articleno = {1055},
  numpages = {29},
  keywords = {Video Accessibility, Video Description, Video Understanding, Blind and Low Vision Users, Multimodal Large Language Models},
}